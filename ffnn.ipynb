{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import sklearn\n",
    "import random\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS CLEANING, TOKENIZATION AND BUILDING VOCAB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPrideAndPrejudice(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    start_idx = text.find(\"CHAPTER I.\")\n",
    "    if start_idx == -1:\n",
    "        return text\n",
    "    text = text[start_idx:]\n",
    "\n",
    "    end_idx = text.find(\"Transcriber's note:\")\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "\n",
    "    text = re.sub(r\"CHAPTER\\s+[IVXLCDM]+\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleanUllyeses(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    first_idx = text.find(\"— I —\")\n",
    "    if first_idx == -1:\n",
    "        return text\n",
    "\n",
    "    second_idx = text.find(\"— I —\", first_idx + 1)\n",
    "    if second_idx == -1:\n",
    "        return text\n",
    "    text = text[second_idx:]\n",
    "\n",
    "    text = re.sub(r\"—\\s+[I|II|III]+\\s+—\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\[\\s*\\d+\\s*\\]\", \"\", text)\n",
    "\n",
    "    end_idx = text.find(\"Trieste-Zurich-Paris\")\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "\n",
    "    return text\n",
    "\n",
    "def Tokenizer(inputText):\n",
    "    text = inputText.lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"http\\S+\", \"<URL>\", text)\n",
    "    text = re.sub(r\"www\\S+\", \"<URL>\", text)\n",
    "    text = re.sub(r\"[A-Za-z0-9._%+-]+@[A-za-z0-9.-]+\\.[a-z]{2,}\", \"<MAILID>\", text)\n",
    "\n",
    "    text = re.sub(r\"[^\\@\\#\\.\\w\\?\\!\\s:-]\", \"\", text)\n",
    "    text = re.sub(f\"-\", \" \", text)\n",
    "    text = re.sub(r\"_\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\n*\", \"\", text)\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "\n",
    "    abbreviations = re.findall(r\"\\b([A-Z]([a-z]){,2}\\.)\", text)\n",
    "    if abbreviations:\n",
    "        abbreviations_set = set((list(zip(*abbreviations))[0]))\n",
    "\n",
    "        for word in abbreviations_set:\n",
    "            pattern = r\"\\b\" + re.escape(word)\n",
    "            text = re.sub(pattern, word.strip(\".\"), text)\n",
    "\n",
    "    text = re.sub(r\"#\\w+\\b\", \"<HASHTAG>\", text)\n",
    "    text = re.sub(r\"@\\w+\\b\", \"<MENTION>\", text)\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"<NUM>\", text)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in re.split(r\"[.!?:]+\", text) if sentence.strip()\n",
    "    ]\n",
    "    sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def trainTestSplit(sentences, testSize):\n",
    "    random.seed(69)\n",
    "\n",
    "    testSplit = random.sample(sentences, testSize)\n",
    "    duplicateTestSplit = testSplit.copy()\n",
    "    trainSplit = [\n",
    "        sentence\n",
    "        for sentence in sentences\n",
    "        if sentence not in duplicateTestSplit or duplicateTestSplit.remove(sentence)\n",
    "    ]\n",
    "\n",
    "    return trainSplit, testSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = input(\"Enter Corpus Path: \")\n",
    "# n = input(\"Enter value of N: \")\n",
    "\n",
    "file_path = 'corpus/corpus_2.txt'\n",
    "n = 3\n",
    "filteredText = \"\"\n",
    "corp = \"\"\n",
    "\n",
    "# corpus cleaning and tokenization\n",
    "if 'corpus_1.txt' in file_path:\n",
    "    filteredText = cleanPrideAndPrejudice(file_path)\n",
    "    corp = \"papc\"\n",
    "elif 'corpus_2.txt' in file_path:\n",
    "    filteredText = cleanUllyeses(file_path)\n",
    "    corp = \"uc\"\n",
    "else:\n",
    "    print(\"Corpus doesn't exist!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = Tokenizer(filteredText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text = trainTestSplit(sentences, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_text))\n",
    "print(len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(word for sentence in train_text for word in sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab.keys())}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "word_to_idx[\"<UNK>\"] = len(word_to_idx)\n",
    "idx_to_word[len(idx_to_word)] = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vocab Size: {len(word_to_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self, tokenized_sentences, n):\n",
    "        self.n = n\n",
    "        \n",
    "        word_counts = Counter([word for sentence in tokenized_sentences for word in sentence])\n",
    "        \n",
    "        # Filter vocabulary based on minimum frequency\n",
    "        frequent_words = {word for word, count in word_counts.items()}\n",
    "        \n",
    "        self.vocab = [\"<PAD>\", \"<UNK>\"] + list(frequent_words)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        self.data = []\n",
    "        pad_idx = self.word2idx[\"<PAD>\"]\n",
    "        unk_idx = self.word2idx[\"<UNK>\"]\n",
    "        \n",
    "        for sentence in tokenized_sentences:\n",
    "            indices = []\n",
    "            for word in sentence:\n",
    "                if word in self.word2idx:\n",
    "                    indices.append(self.word2idx[word])\n",
    "                else:\n",
    "                    indices.append(unk_idx)\n",
    "            \n",
    "            # Add padding at the beginning\n",
    "            padded = [pad_idx] * (n - 1) + indices\n",
    "            \n",
    "            # Create n-gram samples\n",
    "            for i in range(len(indices)):\n",
    "                context = padded[i:i + n - 1]\n",
    "                target = indices[i]\n",
    "                self.data.append((context, target))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context), torch.tensor(target)\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embed all context words: [batch_size, context_size, embedding_dim]\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Flatten the embeddings: [batch_size, context_size * embedding_dim]\n",
    "        batch_size = embedded.size(0)\n",
    "        flattened = embedded.view(batch_size, -1)\n",
    "        \n",
    "        # Pass through layers\n",
    "        hidden = self.activation(self.hidden(flattened))\n",
    "        output = self.output(hidden)\n",
    "        return output\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        print(f\"Model moved to {device}\")\n",
    "    \n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def predict_next_words(self, input_tokens, dataset, k=5):\n",
    "        \"\"\"\n",
    "        Predict next words given a list of input tokens\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        if isinstance(input_tokens, str):\n",
    "            tokens = input_tokens.strip().split()\n",
    "        else:\n",
    "            tokens = input_tokens\n",
    "        \n",
    "        # Ensure we have enough context\n",
    "        if len(tokens) < dataset.n - 1:\n",
    "            tokens = [\"<PAD>\"] * (dataset.n - 1 - len(tokens)) + tokens\n",
    "        elif len(tokens) > dataset.n - 1:\n",
    "            tokens = tokens[-(dataset.n - 1):]\n",
    "        \n",
    "        # Convert tokens to indices, handling OOV\n",
    "        word_indices = []\n",
    "        for word in tokens:\n",
    "            if word in dataset.word2idx:\n",
    "                word_indices.append(dataset.word2idx[word])\n",
    "            else:\n",
    "                word_indices.append(dataset.word2idx[\"<UNK>\"])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.tensor(word_indices).unsqueeze(0).to(self.device)\n",
    "            output = self.model(input_tensor)\n",
    "            \n",
    "            probabilities = torch.softmax(output[0], dim=0)\n",
    "            top_k_probs, top_k_indices = torch.topk(probabilities, k)\n",
    "            \n",
    "            predictions = []\n",
    "            for prob, idx in zip(top_k_probs.cpu().numpy(), top_k_indices.cpu().numpy()):\n",
    "                word = dataset.idx2word[idx]\n",
    "                predictions.append((word, prob))\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def train_language_model(tokenized_sentences, embedding_dim, hidden_dim, n, \n",
    "                        batch_size, num_epochs, device):\n",
    "    # Create dataset\n",
    "    dataset = NGramDataset(tokenized_sentences, n)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(f\"Vocabulary size: {len(dataset.vocab)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FFNN(\n",
    "        vocab_size=len(dataset.vocab),\n",
    "        embedding_dim=embedding_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        context_size=n-1\n",
    "    )\n",
    "    trainer = LanguageModelTrainer(model, device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        loss = trainer.train_epoch(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {loss:.4f}\")\n",
    "    \n",
    "    return model, dataset, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparams\n",
    "# embedding_dimensions = 100\n",
    "# hidden_dimensions = 256\n",
    "# sequence_length = 40\n",
    "# batch_size = 256\n",
    "# epochs = 10\n",
    "\n",
    "# # Train the model\n",
    "# model, dataset, trainer = train_language_model(\n",
    "#     train_text, embedding_dimensions, hidden_dimensions, n, batch_size, epochs, device\n",
    "# )\n",
    "\n",
    "# print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After training, you can make predictions\n",
    "# context = [\"mr\", \"darcy\", \"is\", \"an\"]  # example context\n",
    "# predictions = trainer.predict_next_words(context, dataset, k=10)  # get top 5 predictions\n",
    "\n",
    "# # Print predictions\n",
    "# for word, probability in predictions:\n",
    "#     print(f\"{word}: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_perplexity_and_avg_from_file(model, sentences, dataset, device, batch_size=256, output_file=\"perplexities.txt\"):\n",
    "    model.eval()\n",
    "    sentence_nlls = {}\n",
    "    sentence_lengths = {}\n",
    "    perplexities = []\n",
    "    n = dataset.n  # n-gram size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process each sentence\n",
    "        for sent_idx, sentence in enumerate(sentences):\n",
    "            if len(sentence) < n:  # Skip sentences shorter than n-gram size\n",
    "                continue\n",
    "            \n",
    "            # Convert words to indices\n",
    "            indices = [dataset.word2idx.get(word, dataset.word2idx['<UNK>']) \n",
    "                      for word in sentence]\n",
    "            \n",
    "            # Create n-gram samples for this sentence\n",
    "            contexts = []\n",
    "            targets = []\n",
    "            \n",
    "            for j in range(len(indices) - n + 1):\n",
    "                contexts.append(indices[j:j + n - 1])\n",
    "                targets.append(indices[j + n - 1])\n",
    "            \n",
    "            # Initialize tracking for this sentence\n",
    "            sentence_nlls[sent_idx] = 0\n",
    "            sentence_lengths[sent_idx] = len(targets)\n",
    "            \n",
    "            # Process n-grams in batches\n",
    "            for i in range(0, len(contexts), batch_size):\n",
    "                batch_contexts = contexts[i:i + batch_size]\n",
    "                batch_targets = targets[i:i + batch_size]\n",
    "                \n",
    "                # Convert to tensors and move to device\n",
    "                context_tensor = torch.tensor(batch_contexts, device=device)\n",
    "                target_tensor = torch.tensor(batch_targets, device=device)\n",
    "                \n",
    "                # Get model predictions\n",
    "                output = model(context_tensor)\n",
    "                log_probs = torch.log_softmax(output, dim=-1)\n",
    "                \n",
    "                # Accumulate NLL for this batch\n",
    "                for j in range(len(batch_contexts)):\n",
    "                    nll = -log_probs[j, target_tensor[j]].item()\n",
    "                    sentence_nlls[sent_idx] += nll\n",
    "        \n",
    "        # Calculate and store sentence perplexities\n",
    "        for sent_idx in sorted(sentence_nlls.keys()):\n",
    "            nll = sentence_nlls[sent_idx]\n",
    "            length = sentence_lengths[sent_idx]\n",
    "            ppl = torch.exp(torch.tensor(nll / length)).item()\n",
    "            perplexities.append(f\"sentence {sent_idx+1}: {ppl:.4f}\")\n",
    "    \n",
    "    # Write results to file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(perplexities))\n",
    "    print(f\"Perplexity scores saved to {output_file}\")\n",
    "    \n",
    "    # Now calculate the average perplexity from the file\n",
    "    total_ppl = 0\n",
    "    total_sentences = 0\n",
    "    ppl_list = []\n",
    "    max_ppl = 1e5\n",
    "\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # Extract the perplexity value from the line\n",
    "            try:\n",
    "                sentence_idx, ppl = line.split(\": \")\n",
    "                ppl = float(ppl.strip())\n",
    "\n",
    "                # Clip perplexity values\n",
    "                ppl = min(ppl, max_ppl)\n",
    "                ppl_list.append(ppl)\n",
    "\n",
    "                total_ppl += ppl\n",
    "                total_sentences += 1\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Calculate and return the average perplexity\n",
    "    if total_sentences > 0:\n",
    "        avg_perplexity = total_ppl / total_sentences\n",
    "        return avg_perplexity, ppl_list\n",
    "    else:\n",
    "        print(\"No valid sentences found for perplexity calculation\")\n",
    "        return None, []\n",
    "\n",
    "def plot_perplexity_histogram(train_perplexities, test_perplexities, train_label=\"Train\", test_label=\"Test\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.hist(np.emath.logn(10, train_perplexities), bins=30, alpha=0.5, label=train_label, color='blue', edgecolor='black')\n",
    "    \n",
    "    plt.hist(np.emath.logn(10, test_perplexities), bins=30, alpha=0.5, label=test_label, color='red', edgecolor='black')\n",
    "    \n",
    "    plt.xlabel('Logarithm of Perplexity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Perplexity Distribution')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate perplexities and average perplexity for training set\n",
    "# print(\"Calculating training set perplexity...\")\n",
    "# avg_perplexity_train, train_perplexity_list = calculate_perplexity_and_avg_from_file(\n",
    "#     model=model,\n",
    "#     sentences=train_text,\n",
    "#     dataset=dataset,\n",
    "#     device=device,\n",
    "#     batch_size=128,\n",
    "#     output_file=\"train_perplexities.txt\"\n",
    "# )\n",
    "# print(f\"Average Training Set Perplexity: {avg_perplexity_train}\")\n",
    "\n",
    "# # Calculate perplexities and average perplexity for test set\n",
    "# print(\"\\nCalculating test set perplexity...\")\n",
    "# avg_perplexity_test, test_perplexity_list = calculate_perplexity_and_avg_from_file(\n",
    "#     model=model,\n",
    "#     sentences=test_text,\n",
    "#     dataset=dataset,\n",
    "#     device=device,\n",
    "#     batch_size=128,\n",
    "#     output_file=\"test_perplexities.txt\"\n",
    "# )\n",
    "# print(f\"Average Test Set Perplexity: {avg_perplexity_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_perplexity_histogram(train_perplexity_list, test_perplexity_list, \"Train\", \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(\n",
    "    train_text,\n",
    "    test_text,\n",
    "    hyperparameters,\n",
    "    base_model_dir=\"models_ffnn\",\n",
    "    results_file=\"model_results_ffnn.json\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train multiple models with different hyperparameter combinations and evaluate their perplexities\n",
    "\n",
    "    Args:\n",
    "        train_text: Training data (list of tokenized sentences)\n",
    "        test_text: Test data (list of tokenized sentences)\n",
    "        hyperparameters: Dictionary of hyperparameter lists to try\n",
    "        base_model_dir: Directory to save models\n",
    "        results_file: File to save results\n",
    "    \"\"\"\n",
    "    # Create model directory if it doesn't exist\n",
    "    os.makedirs(base_model_dir, exist_ok=True)\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    param_names = sorted(hyperparameters.keys())\n",
    "    param_values = [hyperparameters[name] for name in param_names]\n",
    "    param_combinations = list(itertools.product(*param_values))\n",
    "\n",
    "    # Store results\n",
    "    results = []\n",
    "\n",
    "    # Train and evaluate each combination\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        param_dict = dict(zip(param_names, params))\n",
    "        print(f\"\\nTraining model {i+1}/{len(param_combinations)}\")\n",
    "        print(\"Parameters:\", param_dict)\n",
    "\n",
    "        # Create model identifier\n",
    "        model_id = f\"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}\"\n",
    "\n",
    "        try:\n",
    "            # Train model\n",
    "            model, dataset, trainer = train_language_model(\n",
    "                train_text,\n",
    "                param_dict[\"embedding_dim\"],\n",
    "                param_dict[\"hidden_dim\"],\n",
    "                param_dict[\"n\"],\n",
    "                param_dict[\"batch_size\"],\n",
    "                param_dict[\"epochs\"],\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            # Calculate perplexities\n",
    "            print(\"\\nCalculating training perplexity...\")\n",
    "            train_perplexity, _ = calculate_perplexity_and_avg_from_file(\n",
    "                model=model,\n",
    "                sentences=train_text,\n",
    "                dataset=dataset,\n",
    "                device=device,\n",
    "                output_file=f\"{base_model_dir}/train_perplexities_{model_id}.txt\",\n",
    "            )\n",
    "\n",
    "            print(\"\\nCalculating test perplexity...\")\n",
    "            test_perplexity, _ = calculate_perplexity_and_avg_from_file(\n",
    "                model=model,\n",
    "                sentences=test_text,\n",
    "                dataset=dataset,\n",
    "                device=device,\n",
    "                output_file=f\"{base_model_dir}/test_perplexities_{model_id}.txt\",\n",
    "            )\n",
    "\n",
    "            # Save model\n",
    "            model_path = f\"{base_model_dir}/model_{model_id}.pt\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"hyperparameters\": param_dict,\n",
    "                    \"train_perplexity\": train_perplexity,\n",
    "                    \"test_perplexity\": test_perplexity,\n",
    "                },\n",
    "                model_path,\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            result = {\n",
    "                \"model_id\": model_id,\n",
    "                \"hyperparameters\": param_dict,\n",
    "                \"train_perplexity\": train_perplexity,\n",
    "                \"test_perplexity\": test_perplexity,\n",
    "                \"model_path\": model_path,\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Save results after each model (in case of crashes)\n",
    "            with open(results_file, \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model with parameters {param_dict}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Find best model based on test perplexity\n",
    "    best_model = min(results, key=lambda x: x[\"test_perplexity\"])\n",
    "    print(\"\\nBest Model:\")\n",
    "    print(json.dumps(best_model, indent=4))\n",
    "\n",
    "    return results, best_model\n",
    "\n",
    "\n",
    "# Define hyperparameter grid\n",
    "hyperparameters = {\n",
    "    \"embedding_dim\": [400],\n",
    "    \"hidden_dim\": [512],\n",
    "    \"n\": [3, 5],\n",
    "    \"batch_size\": [256],\n",
    "    \"epochs\": [5, 10, 15],\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results, best_model = train_and_evaluate_models(\n",
    "    train_text=train_text,\n",
    "    test_text=test_text,\n",
    "    hyperparameters=hyperparameters,\n",
    "    base_model_dir=f\"models_ffnn_{corp}\",\n",
    "    results_file=f\"model_results_ffnn_{corp}.json\",\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Total models trained: {len(results)}\")\n",
    "print(\"\\nBest performing model:\")\n",
    "print(f\"Model ID: {best_model['model_id']}\")\n",
    "print(\"Hyperparameters:\", best_model[\"hyperparameters\"])\n",
    "print(f\"Train Perplexity: {best_model['train_perplexity']:.4f}\")\n",
    "print(f\"Test Perplexity: {best_model['test_perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the JSON data\n",
    "with open('model_results_ffnn.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert JSON data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract hyperparameters and perplexity values\n",
    "df['n'] = df['hyperparameters'].apply(lambda x: x['n'])\n",
    "df['epochs'] = df['hyperparameters'].apply(lambda x: x['epochs'])\n",
    "df['train_perplexity'] = df['train_perplexity']\n",
    "df['test_perplexity'] = df['test_perplexity']\n",
    "\n",
    "# Plot 1: n vs Epochs (for different n values)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for n_value in df['n'].unique():\n",
    "    subset = df[df['n'] == n_value]\n",
    "    plt.plot(subset['epochs'], subset['train_perplexity'], marker='o', label=f'n={n_value}')\n",
    "plt.title('Training Perplexity vs Epochs for Different n Values')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: n vs Training Perplexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "for epoch in df['epochs'].unique():\n",
    "    subset = df[df['epochs'] == epoch]\n",
    "    plt.plot(subset['n'], subset['train_perplexity'], marker='o', label=f'Epochs={epoch}')\n",
    "plt.title('Training Perplexity vs n for Different Epochs')\n",
    "plt.xlabel('n (Context Window Size)')\n",
    "plt.ylabel('Training Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: n vs Test Perplexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "for epoch in df['epochs'].unique():\n",
    "    subset = df[df['epochs'] == epoch]\n",
    "    plt.plot(subset['n'], subset['test_perplexity'], marker='o', label=f'Epochs={epoch}')\n",
    "plt.title('Test Perplexity vs n for Different Epochs')\n",
    "plt.xlabel('n (Context Window Size)')\n",
    "plt.ylabel('Test Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Epochs vs Training Perplexity (for different n values)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for n_value in df['n'].unique():\n",
    "    subset = df[df['n'] == n_value]\n",
    "    plt.plot(subset['epochs'], subset['train_perplexity'], marker='o', label=f'n={n_value}')\n",
    "plt.title('Training Perplexity vs Epochs for Different n Values')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 5: Epochs vs Test Perplexity (for different n values)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for n_value in df['n'].unique():\n",
    "    subset = df[df['n'] == n_value]\n",
    "    plt.plot(subset['epochs'], subset['test_perplexity'], marker='o', label=f'n={n_value}')\n",
    "plt.title('Test Perplexity vs Epochs for Different n Values')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
