{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPrideAndPrejudice(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    start_idx = text.find(\"CHAPTER I.\")\n",
    "    if start_idx == -1:\n",
    "        return text\n",
    "    text = text[start_idx:]\n",
    "\n",
    "    end_idx = text.find(\"Transcriber's note:\")\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "\n",
    "    text = re.sub(r\"CHAPTER\\s+[IVXLCDM]+\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleanUllyeses(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    first_idx = text.find(\"— I —\")\n",
    "    if first_idx == -1:\n",
    "        return text\n",
    "\n",
    "    second_idx = text.find(\"— I —\", first_idx + 1)\n",
    "    if second_idx == -1:\n",
    "        return text\n",
    "    text = text[second_idx:]\n",
    "\n",
    "    text = re.sub(r\"—\\s+[I|II|III]+\\s+—\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\[\\s*\\d+\\s*\\]\", \"\", text)\n",
    "\n",
    "    end_idx = text.find(\"Trieste-Zurich-Paris\")\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "\n",
    "    return text\n",
    "\n",
    "def Tokenizer(inputText):\n",
    "    text = inputText.lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"http\\S+\", \"<URL>\", text)\n",
    "    text = re.sub(r\"www\\S+\", \"<URL>\", text)\n",
    "    text = re.sub(r\"[A-Za-z0-9._%+-]+@[A-za-z0-9.-]+\\.[a-z]{2,}\", \"<MAILID>\", text)\n",
    "\n",
    "    text = re.sub(r\"[^\\@\\#\\.\\w\\?\\!\\s:-]\", \"\", text)\n",
    "    text = re.sub(f\"-\", \" \", text)\n",
    "    text = re.sub(r\"_\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\n*\", \"\", text)\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "\n",
    "    abbreviations = re.findall(r\"\\b([A-Z]([a-z]){,2}\\.)\", text)\n",
    "    if abbreviations:\n",
    "        abbreviations_set = set((list(zip(*abbreviations))[0]))\n",
    "\n",
    "        for word in abbreviations_set:\n",
    "            pattern = r\"\\b\" + re.escape(word)\n",
    "            text = re.sub(pattern, word.strip(\".\"), text)\n",
    "\n",
    "    text = re.sub(r\"#\\w+\\b\", \"<HASHTAG>\", text)\n",
    "    text = re.sub(r\"@\\w+\\b\", \"<MENTION>\", text)\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"<NUM>\", text)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in re.split(r\"[.!?:]+\", text) if sentence.strip()\n",
    "    ]\n",
    "    sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def trainTestSplit(sentences, testSize):\n",
    "    random.seed(69)\n",
    "\n",
    "    testSplit = random.sample(sentences, testSize)\n",
    "    duplicateTestSplit = testSplit.copy()\n",
    "    trainSplit = [\n",
    "        sentence\n",
    "        for sentence in sentences\n",
    "        if sentence not in duplicateTestSplit or duplicateTestSplit.remove(sentence)\n",
    "    ]\n",
    "\n",
    "    return trainSplit, testSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = input(\"Enter Corpus Path: \")\n",
    "# n = input(\"Enter value of N: \")\n",
    "\n",
    "file_path = 'corpus/corpus_2.txt'\n",
    "filteredText = \"\"\n",
    "corp = \"\"\n",
    "\n",
    "# corpus cleaning and tokenization\n",
    "if 'corpus_1.txt' in file_path:\n",
    "    filteredText = cleanPrideAndPrejudice(file_path)\n",
    "    corp = \"papc\"\n",
    "elif 'corpus_2.txt' in file_path:\n",
    "    filteredText = cleanUllyeses(file_path)\n",
    "    corp = \"uc\"\n",
    "else:\n",
    "    print(\"Corpus doesn't exist!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = Tokenizer(filteredText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text = trainTestSplit(sentences, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12906"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(sentence) for sentence in train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram saved at sentence_length_uc.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sentence_length_histogram(sentences, save_path=\"sentence_length_histogram.png\"):\n",
    "    \"\"\"\n",
    "    Plots and saves a histogram of sentence lengths.\n",
    "    Args:\n",
    "        sentences (list of list of str): Tokenized sentences.\n",
    "        save_path (str): Path to save the histogram image.\n",
    "    \"\"\"\n",
    "    # Compute sentence lengths\n",
    "    lengths = [np.log(len(sentence)) for sentence in sentences]\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel(\"Sentence Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of Sentence Lengths\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save and show\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Histogram saved at {save_path}\")\n",
    "\n",
    "plot_sentence_length_histogram(train_text, f\"sentence_length_{corp}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26744\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text))\n",
    "print(len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(word for sentence in train_text for word in sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab.keys())}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "word_to_idx[\"<UNK>\"] = len(word_to_idx)\n",
    "idx_to_word[len(idx_to_word)] = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5960\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5960\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab Size: {len(word_to_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_sentences):\n",
    "        self.seq_length = max(len(sentence) for sentence in tokenized_sentences)\n",
    "        \n",
    "        # Flatten list of lists for vocabulary creation\n",
    "        all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "        \n",
    "        # Create vocabulary\n",
    "        word_counts = Counter(all_words)\n",
    "        self.vocab = ['<PAD>', '<UNK>'] + list(word_counts.keys())\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        # Convert sentences to indices and flatten\n",
    "        self.data = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            sentence_indices = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in sentence]\n",
    "            self.data.extend(sentence_indices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.data) - self.seq_length - 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx:idx + self.seq_length]\n",
    "        target = self.data[idx + 1:idx + self.seq_length + 1]\n",
    "        return torch.tensor(sequence), torch.tensor(target)\n",
    "    \n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(1, batch_size, self.hidden_dim, device=x.device)\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_dim, device=device)\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        print(f\"Model moved to {device}\")\n",
    "        \n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            hidden = self.model.init_hidden(data.size(0))\n",
    "            output, _ = self.model(data, hidden)\n",
    "            \n",
    "            loss = self.criterion(output.reshape(-1, output.size(-1)), target.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def predict_next_words(self, input_tokens, dataset, k=5):\n",
    "        \"\"\"\n",
    "        Predict next words given a list of input tokens\n",
    "        Args:\n",
    "            input_tokens: List of tokens or space-separated string\n",
    "            dataset: TextDataset instance\n",
    "            k: Number of top predictions to return\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Handle both string and list input\n",
    "        if isinstance(input_tokens, str):\n",
    "            tokens = input_tokens.strip().split()\n",
    "        else:\n",
    "            tokens = input_tokens\n",
    "            \n",
    "        # Convert tokens to indices\n",
    "        word_indices = [dataset.word2idx.get(word, dataset.word2idx['<UNK>']) for word in tokens]\n",
    "        \n",
    "        # # Trim sequence if longer than seq_length\n",
    "        # if len(word_indices) > dataset.seq_length:\n",
    "        #     word_indices = word_indices[-dataset.seq_length:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.tensor(word_indices).unsqueeze(0).to(device)\n",
    "            hidden = self.model.init_hidden(1)\n",
    "            output, _ = self.model(input_tensor, hidden)\n",
    "            \n",
    "            probabilities = torch.softmax(output[0, -1], dim=0)\n",
    "            top_k_probs, top_k_indices = torch.topk(probabilities, k)\n",
    "            \n",
    "            predictions = []\n",
    "            for prob, idx in zip(top_k_probs.cpu().numpy(), top_k_indices.cpu().numpy()):\n",
    "                word = dataset.idx2word[idx]\n",
    "                predictions.append((word, prob))\n",
    "                \n",
    "        return predictions\n",
    "\n",
    "def train_language_model(tokenized_sentences, embedding_dim, hidden_dim, \n",
    "                        batch_size, num_epochs):\n",
    "    # Print memory usage before training\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory before training: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TextDataset(tokenized_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(f\"Vocabulary size: {len(dataset.vocab)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = VanillaRNN(len(dataset.vocab), embedding_dim, hidden_dim)\n",
    "    trainer = LanguageModelTrainer(model)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        loss = trainer.train_epoch(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {loss:.4f}')\n",
    "        \n",
    "        # Print memory usage after each epoch\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "    \n",
    "    return model, dataset, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparams\n",
    "# embedding_dimensions = 200\n",
    "# hidden_dimensions = 512\n",
    "# batch_size = 256\n",
    "# epochs = 5\n",
    "\n",
    "# model, dataset, trainer = train_language_model(train_text, embedding_dimensions, hidden_dimensions, batch_size, epochs)\n",
    "# print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using string\n",
    "# predictions = trainer.predict_next_words(\"mr darcy is the real\", dataset, k=10)\n",
    "\n",
    "# print(\"\\nTest predictions:\")\n",
    "# for word, prob in predictions:\n",
    "#     print(f'Word: {word}, Probability: {prob:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, sentences, dataset, device, batch_size=64, output_file=\"perplexities.txt\"):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of tokenized sentences using batch processing\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_nll = 0\n",
    "    total_words = 0\n",
    "    perplexities = []\n",
    "    \n",
    "    # Process sentences in batches\n",
    "    with torch.no_grad():\n",
    "        # Convert all sentences to tensors first\n",
    "        batch_data = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if len(sentence) < 2:  # Skip too short sentences\n",
    "                continue\n",
    "                \n",
    "            indices = [dataset.word2idx.get(word, dataset.word2idx['<UNK>']) for word in sentence]\n",
    "            batch_data.append((i, indices))\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(batch_data), batch_size):\n",
    "            batch = batch_data[i:i + batch_size]\n",
    "            batch_indices = []\n",
    "            batch_targets = []\n",
    "            sentence_lengths = []\n",
    "            original_indices = []\n",
    "            \n",
    "            # Prepare batch data\n",
    "            for idx, indices in batch:\n",
    "                original_indices.append(idx)\n",
    "                batch_indices.append(indices[:-1])  # All but last word\n",
    "                batch_targets.append(indices[1:])   # All but first word\n",
    "                sentence_lengths.append(len(indices) - 1)\n",
    "            \n",
    "            # Pad sequences in batch\n",
    "            max_len = max(sentence_lengths)\n",
    "            padded_indices = [seq + [dataset.word2idx['<PAD>']] * (max_len - len(seq)) for seq in batch_indices]\n",
    "            padded_targets = [seq + [dataset.word2idx['<PAD>']] * (max_len - len(seq)) for seq in batch_targets]\n",
    "            \n",
    "            # Convert to tensors\n",
    "            context_tensor = torch.tensor(padded_indices).to(device)\n",
    "            target_tensor = torch.tensor(padded_targets).to(device)\n",
    "            lengths_tensor = torch.tensor(sentence_lengths).to(device)\n",
    "            \n",
    "            # Get model output\n",
    "            output, _ = model(context_tensor)\n",
    "            \n",
    "            # Calculate log probabilities\n",
    "            log_probs = torch.log_softmax(output, dim=-1)\n",
    "            \n",
    "            # Calculate perplexity for each sentence in batch\n",
    "            for j in range(len(batch)):\n",
    "                length = sentence_lengths[j]\n",
    "                sentence_log_probs = log_probs[j, :length]\n",
    "                sentence_targets = target_tensor[j, :length]\n",
    "                \n",
    "                # Calculate NLL for this sentence\n",
    "                nll = 0\n",
    "                for t in range(length):\n",
    "                    nll += -sentence_log_probs[t, sentence_targets[t]]\n",
    "                \n",
    "                # Calculate perplexity\n",
    "                ppl_score = torch.exp(nll / length).item()\n",
    "                perplexities.append(f\"sentence {original_indices[j]+1}: {ppl_score:.4f}\")\n",
    "                \n",
    "                # Update totals\n",
    "                total_nll += nll.item()\n",
    "                total_words += length\n",
    "            \n",
    "    # Write results to file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(perplexities))\n",
    "    print(f\"Perplexity scores saved to {output_file}\")\n",
    "    \n",
    "    # Calculate average perplexity\n",
    "    if total_words > 0:\n",
    "        avg_nll = total_nll / total_words\n",
    "        avg_perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
    "        print(f\"Average Perplexity: {avg_perplexity:.4f}\")\n",
    "        return avg_perplexity\n",
    "    else:\n",
    "        print(\"No valid sentences found for perplexity calculation\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate perplexities for both sets\n",
    "# print(\"Calculating training set perplexity...\")\n",
    "# train_perplexity = calculate_perplexity(\n",
    "#     model=model,\n",
    "#     sentences=train_text,\n",
    "#     dataset=dataset,\n",
    "#     device=device,\n",
    "#     batch_size=128,\n",
    "#     output_file=\"train_perplexities.txt\"\n",
    "# )\n",
    "\n",
    "# print(\"\\nCalculating test set perplexity...\")\n",
    "# test_perplexity = calculate_perplexity(\n",
    "#     model=model,\n",
    "#     sentences=test_text,  # Your test sentences\n",
    "#     dataset=dataset,\n",
    "#     device=device,\n",
    "#     batch_size=128,  # Adjust based on your GPU memory\n",
    "#     output_file=\"test_perplexities.txt\"\n",
    "# )\n",
    "\n",
    "# print(f\"\\nFinal Results:\")\n",
    "# print(f\"Training Set Perplexity: {train_perplexity:.4f}\")\n",
    "# print(f\"Test Set Perplexity: {test_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 1/9\n",
      "Parameters: {'batch_size': 256, 'embedding_dim': 50, 'epochs': 5, 'hidden_dim': 512}\n",
      "GPU Memory before training: 0.00 MB\n",
      "Vocabulary size: 5961\n",
      "Model moved to cuda\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 8.7046\n",
      "Batch 100, Loss: 6.3176\n",
      "Batch 200, Loss: 6.2659\n",
      "Batch 300, Loss: 6.0942\n",
      "Batch 400, Loss: 5.9544\n",
      "Epoch 1/5, Average Loss: 6.3965\n",
      "GPU Memory: 72.14 MB\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 5.9249\n",
      "Batch 100, Loss: 5.8183\n",
      "Batch 200, Loss: 5.6287\n",
      "Batch 300, Loss: 5.5388\n",
      "Batch 400, Loss: 5.4269\n",
      "Epoch 2/5, Average Loss: 5.6514\n",
      "GPU Memory: 72.14 MB\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 5.4065\n",
      "Batch 100, Loss: 5.2949\n",
      "Batch 200, Loss: 5.2128\n",
      "Batch 300, Loss: 5.1568\n",
      "Batch 400, Loss: 5.0636\n",
      "Epoch 3/5, Average Loss: 5.2170\n",
      "GPU Memory: 72.14 MB\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 5.0399\n",
      "Batch 100, Loss: 4.9574\n",
      "Batch 200, Loss: 4.8481\n",
      "Batch 300, Loss: 4.7573\n",
      "Batch 400, Loss: 4.6740\n",
      "Epoch 4/5, Average Loss: 4.8497\n",
      "GPU Memory: 72.14 MB\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 4.6203\n",
      "Batch 100, Loss: 4.5512\n",
      "Batch 200, Loss: 4.4565\n",
      "Batch 300, Loss: 4.3801\n",
      "Batch 400, Loss: 4.2722\n",
      "Epoch 5/5, Average Loss: 4.4607\n",
      "GPU Memory: 72.14 MB\n",
      "\n",
      "Calculating training perplexity...\n",
      "Perplexity scores saved to models_rnn/train_perplexities_model_20250214_162748_0.txt\n",
      "Average Perplexity: 79.6391\n",
      "\n",
      "Calculating test perplexity...\n",
      "Perplexity scores saved to models_rnn/test_perplexities_model_20250214_162748_0.txt\n",
      "Average Perplexity: 282.3174\n",
      "\n",
      "Training model 2/9\n",
      "Parameters: {'batch_size': 256, 'embedding_dim': 50, 'epochs': 6, 'hidden_dim': 512}\n",
      "GPU Memory before training: 72.14 MB\n",
      "Vocabulary size: 5961\n",
      "Model moved to cuda\n",
      "\n",
      "Epoch 1/6\n",
      "Batch 0, Loss: 8.7025\n",
      "Batch 100, Loss: 6.3280\n",
      "Batch 200, Loss: 6.2807\n",
      "Batch 300, Loss: 6.1240\n",
      "Batch 400, Loss: 5.9862\n",
      "Epoch 1/6, Average Loss: 6.4008\n",
      "GPU Memory: 129.20 MB\n",
      "\n",
      "Epoch 2/6\n",
      "Batch 0, Loss: 5.9537\n",
      "Batch 100, Loss: 5.7918\n",
      "Batch 200, Loss: 5.6178\n",
      "Batch 300, Loss: 5.4841\n",
      "Batch 400, Loss: 5.3765\n",
      "Epoch 2/6, Average Loss: 5.6269\n",
      "GPU Memory: 129.20 MB\n",
      "\n",
      "Epoch 3/6\n",
      "Batch 0, Loss: 5.3535\n",
      "Batch 100, Loss: 5.2818\n",
      "Batch 200, Loss: 5.1613\n",
      "Batch 300, Loss: 5.0881\n",
      "Batch 400, Loss: 5.0321\n",
      "Epoch 3/6, Average Loss: 5.1743\n",
      "GPU Memory: 129.20 MB\n",
      "\n",
      "Epoch 4/6\n",
      "Batch 0, Loss: 5.0038\n",
      "Batch 100, Loss: 4.9417\n",
      "Batch 200, Loss: 4.8725\n",
      "Batch 300, Loss: 4.7792\n",
      "Batch 400, Loss: 4.6759\n",
      "Epoch 4/6, Average Loss: 4.8542\n",
      "GPU Memory: 129.20 MB\n",
      "\n",
      "Epoch 5/6\n",
      "Batch 0, Loss: 4.6872\n",
      "Batch 100, Loss: 4.6128\n",
      "Batch 200, Loss: 4.5433\n",
      "Batch 300, Loss: 4.4668\n",
      "Batch 400, Loss: 4.3909\n",
      "Epoch 5/6, Average Loss: 4.5354\n",
      "GPU Memory: 129.20 MB\n",
      "\n",
      "Epoch 6/6\n",
      "Batch 0, Loss: 4.4104\n",
      "Batch 100, Loss: 4.3082\n",
      "Batch 200, Loss: 4.2290\n",
      "Batch 300, Loss: 4.1727\n",
      "Batch 400, Loss: 4.0769\n",
      "Epoch 6/6, Average Loss: 4.2340\n",
      "GPU Memory: 129.20 MB\n",
      "\n",
      "Calculating training perplexity...\n",
      "Perplexity scores saved to models_rnn/train_perplexities_model_20250214_163450_1.txt\n",
      "Average Perplexity: 63.8071\n",
      "\n",
      "Calculating test perplexity...\n",
      "Perplexity scores saved to models_rnn/test_perplexities_model_20250214_163450_1.txt\n",
      "Average Perplexity: 289.9980\n",
      "\n",
      "Training model 3/9\n",
      "Parameters: {'batch_size': 256, 'embedding_dim': 50, 'epochs': 7, 'hidden_dim': 512}\n",
      "GPU Memory before training: 73.32 MB\n",
      "Vocabulary size: 5961\n",
      "Model moved to cuda\n",
      "\n",
      "Epoch 1/7\n",
      "Batch 0, Loss: 8.7055\n",
      "Batch 100, Loss: 6.3245\n",
      "Batch 200, Loss: 6.2883\n",
      "Batch 300, Loss: 6.1684\n",
      "Batch 400, Loss: 6.0105\n",
      "Epoch 1/7, Average Loss: 6.4098\n",
      "GPU Memory: 130.13 MB\n",
      "\n",
      "Epoch 2/7\n",
      "Batch 0, Loss: 5.9968\n",
      "Batch 100, Loss: 5.8461\n",
      "Batch 200, Loss: 5.6620\n",
      "Batch 300, Loss: 5.5583\n",
      "Batch 400, Loss: 5.4739\n",
      "Epoch 2/7, Average Loss: 5.6906\n",
      "GPU Memory: 130.13 MB\n",
      "\n",
      "Epoch 3/7\n",
      "Batch 0, Loss: 5.4732\n",
      "Batch 100, Loss: 5.3812\n",
      "Batch 200, Loss: 5.3072\n",
      "Batch 300, Loss: 5.1881\n",
      "Batch 400, Loss: 5.0970\n",
      "Epoch 3/7, Average Loss: 5.2856\n",
      "GPU Memory: 130.13 MB\n",
      "\n",
      "Epoch 4/7\n",
      "Batch 0, Loss: 5.1002\n",
      "Batch 100, Loss: 4.9783\n",
      "Batch 200, Loss: 4.9242\n",
      "Batch 300, Loss: 4.8349\n",
      "Batch 400, Loss: 4.7507\n",
      "Epoch 4/7, Average Loss: 4.9101\n",
      "GPU Memory: 130.13 MB\n",
      "\n",
      "Epoch 5/7\n",
      "Batch 0, Loss: 4.7356\n",
      "Batch 100, Loss: 4.6340\n",
      "Batch 200, Loss: 4.5596\n",
      "Batch 300, Loss: 4.4876\n",
      "Batch 400, Loss: 4.4087\n",
      "Epoch 5/7, Average Loss: 4.5540\n",
      "GPU Memory: 130.13 MB\n",
      "\n",
      "Epoch 6/7\n",
      "Batch 0, Loss: 4.3668\n",
      "Batch 100, Loss: 4.2957\n",
      "Batch 200, Loss: 4.2136\n",
      "Batch 300, Loss: 4.1110\n",
      "Batch 400, Loss: 4.0541\n",
      "Epoch 6/7, Average Loss: 4.2089\n",
      "GPU Memory: 130.13 MB\n",
      "\n",
      "Epoch 7/7\n",
      "Batch 0, Loss: 4.0261\n",
      "Batch 100, Loss: 3.9470\n",
      "Batch 200, Loss: 3.8889\n",
      "Batch 300, Loss: 3.7993\n",
      "Batch 400, Loss: 3.7324\n",
      "Epoch 7/7, Average Loss: 3.8690\n",
      "GPU Memory: 130.13 MB\n",
      "\n",
      "Calculating training perplexity...\n",
      "Perplexity scores saved to models_rnn/train_perplexities_model_20250214_164313_2.txt\n",
      "Average Perplexity: 45.7977\n",
      "\n",
      "Calculating test perplexity...\n",
      "Perplexity scores saved to models_rnn/test_perplexities_model_20250214_164313_2.txt\n",
      "Average Perplexity: 310.4858\n",
      "\n",
      "Training model 4/9\n",
      "Parameters: {'batch_size': 256, 'embedding_dim': 100, 'epochs': 5, 'hidden_dim': 512}\n",
      "GPU Memory before training: 73.06 MB\n",
      "Vocabulary size: 5961\n",
      "Model moved to cuda\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 8.7038\n",
      "Batch 100, Loss: 6.3167\n",
      "Batch 200, Loss: 6.1719\n",
      "Batch 300, Loss: 5.9130\n",
      "Batch 400, Loss: 5.7113\n",
      "Epoch 1/5, Average Loss: 6.2777\n",
      "GPU Memory: 134.26 MB\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 5.6674\n",
      "Batch 100, Loss: 5.4842\n",
      "Batch 200, Loss: 5.2994\n",
      "Batch 300, Loss: 5.1787\n",
      "Batch 400, Loss: 5.0278\n",
      "Epoch 2/5, Average Loss: 5.3207\n",
      "GPU Memory: 134.26 MB\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 5.0138\n",
      "Batch 100, Loss: 4.8953\n",
      "Batch 200, Loss: 4.7941\n",
      "Batch 300, Loss: 4.6798\n",
      "Batch 400, Loss: 4.5650\n",
      "Epoch 3/5, Average Loss: 4.7881\n",
      "GPU Memory: 134.26 MB\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 4.5416\n",
      "Batch 100, Loss: 4.4573\n",
      "Batch 200, Loss: 4.3481\n",
      "Batch 300, Loss: 4.2264\n",
      "Batch 400, Loss: 4.1213\n",
      "Epoch 4/5, Average Loss: 4.3362\n",
      "GPU Memory: 134.26 MB\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 4.0910\n",
      "Batch 100, Loss: 4.0039\n",
      "Batch 200, Loss: 3.9198\n",
      "Batch 300, Loss: 3.8214\n",
      "Batch 400, Loss: 3.7098\n",
      "Epoch 5/5, Average Loss: 3.9076\n",
      "GPU Memory: 134.26 MB\n",
      "\n",
      "Calculating training perplexity...\n",
      "Perplexity scores saved to models_rnn/train_perplexities_model_20250214_165312_3.txt\n",
      "Average Perplexity: 44.9988\n",
      "\n",
      "Calculating test perplexity...\n",
      "Perplexity scores saved to models_rnn/test_perplexities_model_20250214_165312_3.txt\n",
      "Average Perplexity: 261.2149\n",
      "\n",
      "Training model 5/9\n",
      "Parameters: {'batch_size': 256, 'embedding_dim': 100, 'epochs': 6, 'hidden_dim': 512}\n",
      "GPU Memory before training: 77.45 MB\n",
      "Vocabulary size: 5961\n",
      "Model moved to cuda\n",
      "\n",
      "Epoch 1/6\n",
      "Batch 0, Loss: 8.7074\n",
      "Batch 100, Loss: 6.3294\n",
      "Batch 200, Loss: 6.1684\n",
      "Batch 300, Loss: 5.9924\n",
      "Batch 400, Loss: 5.7247\n",
      "Epoch 1/6, Average Loss: 6.2993\n",
      "GPU Memory: 139.20 MB\n",
      "\n",
      "Epoch 2/6\n",
      "Batch 0, Loss: 5.6860\n",
      "Batch 100, Loss: 5.4772\n",
      "Batch 200, Loss: 5.3357\n",
      "Batch 300, Loss: 5.1750\n",
      "Batch 400, Loss: 5.0128\n",
      "Epoch 2/6, Average Loss: 5.3312\n",
      "GPU Memory: 139.20 MB\n",
      "\n",
      "Epoch 3/6\n",
      "Batch 0, Loss: 5.0237\n",
      "Batch 100, Loss: 4.8968\n",
      "Batch 200, Loss: 4.7907\n",
      "Batch 300, Loss: 4.6273\n",
      "Batch 400, Loss: 4.5582\n",
      "Epoch 3/6, Average Loss: 4.7779\n",
      "GPU Memory: 139.20 MB\n",
      "\n",
      "Epoch 4/6\n",
      "Batch 0, Loss: 4.5273\n",
      "Batch 100, Loss: 4.4090\n",
      "Batch 200, Loss: 4.3025\n",
      "Batch 300, Loss: 4.1825\n",
      "Batch 400, Loss: 4.0884\n",
      "Epoch 4/6, Average Loss: 4.2978\n",
      "GPU Memory: 139.20 MB\n",
      "\n",
      "Epoch 5/6\n",
      "Batch 0, Loss: 4.0569\n",
      "Batch 100, Loss: 3.9643\n",
      "Batch 200, Loss: 3.8590\n",
      "Batch 300, Loss: 3.7621\n",
      "Batch 400, Loss: 3.6559\n",
      "Epoch 5/6, Average Loss: 3.8563\n",
      "GPU Memory: 139.20 MB\n",
      "\n",
      "Epoch 6/6\n",
      "Batch 0, Loss: 3.6373\n",
      "Batch 100, Loss: 3.5369\n",
      "Batch 200, Loss: 3.4444\n",
      "Batch 300, Loss: 3.3797\n",
      "Batch 400, Loss: 3.2920\n",
      "Epoch 6/6, Average Loss: 3.4585\n",
      "GPU Memory: 139.20 MB\n",
      "\n",
      "Calculating training perplexity...\n",
      "Perplexity scores saved to models_rnn/train_perplexities_model_20250214_170044_4.txt\n",
      "Average Perplexity: 30.4702\n",
      "\n",
      "Calculating test perplexity...\n",
      "Perplexity scores saved to models_rnn/test_perplexities_model_20250214_170044_4.txt\n",
      "Average Perplexity: 285.9684\n",
      "\n",
      "Training model 6/9\n",
      "Parameters: {'batch_size': 256, 'embedding_dim': 100, 'epochs': 7, 'hidden_dim': 512}\n",
      "GPU Memory before training: 78.00 MB\n",
      "Vocabulary size: 5961\n",
      "Model moved to cuda\n",
      "\n",
      "Epoch 1/7\n",
      "Batch 0, Loss: 8.7053\n",
      "Batch 100, Loss: 6.3052\n",
      "Batch 200, Loss: 6.1792\n",
      "Batch 300, Loss: 5.9391\n",
      "Batch 400, Loss: 5.6848\n",
      "Epoch 1/7, Average Loss: 6.2830\n",
      "GPU Memory: 140.92 MB\n",
      "\n",
      "Epoch 2/7\n",
      "Batch 0, Loss: 5.6689\n",
      "Batch 100, Loss: 5.4498\n",
      "Batch 200, Loss: 5.2935\n",
      "Batch 300, Loss: 5.1407\n",
      "Batch 400, Loss: 5.0123\n",
      "Epoch 2/7, Average Loss: 5.2940\n",
      "GPU Memory: 140.92 MB\n",
      "\n",
      "Epoch 3/7\n",
      "Batch 0, Loss: 4.9896\n",
      "Batch 100, Loss: 4.8662\n",
      "Batch 200, Loss: 4.7700\n",
      "Batch 300, Loss: 4.6489\n",
      "Batch 400, Loss: 4.5307\n",
      "Epoch 3/7, Average Loss: 4.7514\n",
      "GPU Memory: 140.92 MB\n",
      "\n",
      "Epoch 4/7\n",
      "Batch 0, Loss: 4.5130\n",
      "Batch 100, Loss: 4.4240\n",
      "Batch 200, Loss: 4.3095\n",
      "Batch 300, Loss: 4.1823\n",
      "Batch 400, Loss: 4.0501\n",
      "Epoch 4/7, Average Loss: 4.2875\n",
      "GPU Memory: 140.92 MB\n",
      "\n",
      "Epoch 5/7\n",
      "Batch 0, Loss: 4.0606\n",
      "Batch 100, Loss: 3.9608\n",
      "Batch 200, Loss: 3.8611\n",
      "Batch 300, Loss: 3.7571\n",
      "Batch 400, Loss: 3.6905\n",
      "Epoch 5/7, Average Loss: 3.8640\n",
      "GPU Memory: 140.92 MB\n",
      "\n",
      "Epoch 6/7\n",
      "Batch 0, Loss: 3.6575\n",
      "Batch 100, Loss: 3.5618\n",
      "Batch 200, Loss: 3.4931\n",
      "Batch 300, Loss: 3.3800\n",
      "Batch 400, Loss: 3.3102\n",
      "Epoch 6/7, Average Loss: 3.4762\n",
      "GPU Memory: 140.92 MB\n",
      "\n",
      "Epoch 7/7\n",
      "Batch 0, Loss: 3.2963\n",
      "Batch 100, Loss: 3.2157\n",
      "Batch 200, Loss: 3.1481\n",
      "Batch 300, Loss: 3.0507\n",
      "Batch 400, Loss: 2.9864\n",
      "Epoch 7/7, Average Loss: 3.1293\n",
      "GPU Memory: 140.92 MB\n",
      "\n",
      "Calculating training perplexity...\n",
      "Perplexity scores saved to models_rnn/train_perplexities_model_20250214_170905_5.txt\n",
      "Average Perplexity: 23.2078\n",
      "\n",
      "Calculating test perplexity...\n",
      "Perplexity scores saved to models_rnn/test_perplexities_model_20250214_170905_5.txt\n",
      "Average Perplexity: 316.7812\n",
      "\n",
      "Training model 7/9\n",
      "Parameters: {'batch_size': 256, 'embedding_dim': 200, 'epochs': 5, 'hidden_dim': 512}\n",
      "GPU Memory before training: 79.16 MB\n",
      "Vocabulary size: 5961\n",
      "Model moved to cuda\n",
      "\n",
      "Epoch 1/5\n",
      "Batch 0, Loss: 8.7138\n",
      "Batch 100, Loss: 6.2449\n",
      "Batch 200, Loss: 5.9242\n",
      "Batch 300, Loss: 5.5902\n",
      "Batch 400, Loss: 5.3478\n",
      "Epoch 1/5, Average Loss: 6.0998\n",
      "GPU Memory: 151.08 MB\n",
      "\n",
      "Epoch 2/5\n",
      "Batch 0, Loss: 5.3332\n",
      "Batch 100, Loss: 5.1291\n",
      "Batch 200, Loss: 4.9505\n",
      "Batch 300, Loss: 4.8013\n",
      "Batch 400, Loss: 4.6437\n",
      "Epoch 2/5, Average Loss: 4.9539\n",
      "GPU Memory: 151.08 MB\n",
      "\n",
      "Epoch 3/5\n",
      "Batch 0, Loss: 4.6288\n",
      "Batch 100, Loss: 4.4953\n",
      "Batch 200, Loss: 4.3373\n",
      "Batch 300, Loss: 4.2307\n",
      "Batch 400, Loss: 4.1148\n",
      "Epoch 3/5, Average Loss: 4.3577\n",
      "GPU Memory: 151.08 MB\n",
      "\n",
      "Epoch 4/5\n",
      "Batch 0, Loss: 4.0990\n",
      "Batch 100, Loss: 3.9506\n",
      "Batch 200, Loss: 3.8440\n",
      "Batch 300, Loss: 3.7174\n",
      "Batch 400, Loss: 3.6020\n",
      "Epoch 4/5, Average Loss: 3.8310\n",
      "GPU Memory: 151.08 MB\n",
      "\n",
      "Epoch 5/5\n",
      "Batch 0, Loss: 3.5573\n",
      "Batch 100, Loss: 3.4567\n",
      "Batch 200, Loss: 3.3493\n",
      "Batch 300, Loss: 3.2227\n",
      "Batch 400, Loss: 3.1103\n",
      "Epoch 5/5, Average Loss: 3.3392\n",
      "GPU Memory: 151.08 MB\n",
      "\n",
      "Calculating training perplexity...\n",
      "Perplexity scores saved to models_rnn/train_perplexities_model_20250214_171849_6.txt\n",
      "Average Perplexity: 26.0269\n",
      "\n",
      "Calculating test perplexity...\n",
      "Perplexity scores saved to models_rnn/test_perplexities_model_20250214_171849_6.txt\n",
      "Average Perplexity: 259.6877\n",
      "\n",
      "Training model 8/9\n",
      "Parameters: {'batch_size': 256, 'embedding_dim': 200, 'epochs': 6, 'hidden_dim': 512}\n",
      "GPU Memory before training: 88.17 MB\n",
      "Vocabulary size: 5961\n",
      "Model moved to cuda\n",
      "\n",
      "Epoch 1/6\n",
      "Batch 0, Loss: 8.7161\n",
      "Batch 100, Loss: 6.2896\n",
      "Batch 200, Loss: 5.9721\n",
      "Batch 300, Loss: 5.6249\n",
      "Batch 400, Loss: 5.3868\n",
      "Epoch 1/6, Average Loss: 6.1273\n",
      "GPU Memory: 159.23 MB\n",
      "\n",
      "Epoch 2/6\n",
      "Batch 0, Loss: 5.3705\n",
      "Batch 100, Loss: 5.1514\n",
      "Batch 200, Loss: 4.9775\n",
      "Batch 300, Loss: 4.8109\n",
      "Batch 400, Loss: 4.6632\n",
      "Epoch 2/6, Average Loss: 4.9787\n",
      "GPU Memory: 159.23 MB\n",
      "\n",
      "Epoch 3/6\n",
      "Batch 0, Loss: 4.6814\n",
      "Batch 100, Loss: 4.5070\n",
      "Batch 200, Loss: 4.3808\n",
      "Batch 300, Loss: 4.2421\n",
      "Batch 400, Loss: 4.1313\n",
      "Epoch 3/6, Average Loss: 4.3766\n",
      "GPU Memory: 159.23 MB\n",
      "\n",
      "Epoch 4/6\n",
      "Batch 0, Loss: 4.1145\n",
      "Batch 100, Loss: 3.9666\n",
      "Batch 200, Loss: 3.8602\n",
      "Batch 300, Loss: 3.7087\n",
      "Batch 400, Loss: 3.6165\n",
      "Epoch 4/6, Average Loss: 3.8465\n",
      "GPU Memory: 159.23 MB\n",
      "\n",
      "Epoch 5/6\n",
      "Batch 0, Loss: 3.5954\n",
      "Batch 100, Loss: 3.4731\n",
      "Batch 200, Loss: 3.3618\n",
      "Batch 300, Loss: 3.2211\n",
      "Batch 400, Loss: 3.1358\n",
      "Epoch 5/6, Average Loss: 3.3504\n",
      "GPU Memory: 159.23 MB\n",
      "\n",
      "Epoch 6/6\n",
      "Batch 0, Loss: 3.1375\n",
      "Batch 100, Loss: 3.0086\n",
      "Batch 200, Loss: 2.9148\n",
      "Batch 300, Loss: 2.8285\n",
      "Batch 400, Loss: 2.7320\n",
      "Epoch 6/6, Average Loss: 2.9103\n",
      "GPU Memory: 159.23 MB\n",
      "\n",
      "Calculating training perplexity...\n",
      "Perplexity scores saved to models_rnn/train_perplexities_model_20250214_172552_7.txt\n",
      "Average Perplexity: 18.5479\n",
      "\n",
      "Calculating test perplexity...\n",
      "Perplexity scores saved to models_rnn/test_perplexities_model_20250214_172552_7.txt\n",
      "Average Perplexity: 305.5607\n",
      "\n",
      "Training model 9/9\n",
      "Parameters: {'batch_size': 256, 'embedding_dim': 200, 'epochs': 7, 'hidden_dim': 512}\n",
      "GPU Memory before training: 87.32 MB\n",
      "Vocabulary size: 5961\n",
      "Model moved to cuda\n",
      "\n",
      "Epoch 1/7\n",
      "Batch 0, Loss: 8.7146\n",
      "Batch 100, Loss: 6.2782\n",
      "Batch 200, Loss: 5.9680\n",
      "Batch 300, Loss: 5.6236\n",
      "Batch 400, Loss: 5.3695\n",
      "Epoch 1/7, Average Loss: 6.1215\n",
      "GPU Memory: 159.31 MB\n",
      "\n",
      "Epoch 2/7\n",
      "Batch 0, Loss: 5.3539\n",
      "Batch 100, Loss: 5.1553\n",
      "Batch 200, Loss: 4.9813\n",
      "Batch 300, Loss: 4.8347\n",
      "Batch 400, Loss: 4.6739\n",
      "Epoch 2/7, Average Loss: 4.9699\n",
      "GPU Memory: 159.31 MB\n",
      "\n",
      "Epoch 3/7\n",
      "Batch 0, Loss: 4.6572\n",
      "Batch 100, Loss: 4.5020\n",
      "Batch 200, Loss: 4.3783\n",
      "Batch 300, Loss: 4.2712\n",
      "Batch 400, Loss: 4.1342\n",
      "Epoch 3/7, Average Loss: 4.3771\n",
      "GPU Memory: 159.31 MB\n",
      "\n",
      "Epoch 4/7\n",
      "Batch 0, Loss: 4.0956\n",
      "Batch 100, Loss: 3.9785\n",
      "Batch 200, Loss: 3.8594\n",
      "Batch 300, Loss: 3.7451\n",
      "Batch 400, Loss: 3.6186\n",
      "Epoch 4/7, Average Loss: 3.8551\n",
      "GPU Memory: 159.31 MB\n",
      "\n",
      "Epoch 5/7\n",
      "Batch 0, Loss: 3.5860\n",
      "Batch 100, Loss: 3.4841\n",
      "Batch 200, Loss: 3.3652\n",
      "Batch 300, Loss: 3.2657\n",
      "Batch 400, Loss: 3.1609\n",
      "Epoch 5/7, Average Loss: 3.3683\n",
      "GPU Memory: 159.31 MB\n",
      "\n",
      "Epoch 6/7\n",
      "Batch 0, Loss: 3.1425\n",
      "Batch 100, Loss: 3.0502\n",
      "Batch 200, Loss: 2.9059\n",
      "Batch 300, Loss: 2.8256\n",
      "Batch 400, Loss: 2.7487\n",
      "Epoch 6/7, Average Loss: 2.9291\n",
      "GPU Memory: 159.31 MB\n",
      "\n",
      "Epoch 7/7\n",
      "Batch 0, Loss: 2.7571\n",
      "Batch 100, Loss: 2.6683\n",
      "Batch 200, Loss: 2.6050\n",
      "Batch 300, Loss: 2.5174\n",
      "Batch 400, Loss: 2.4145\n",
      "Epoch 7/7, Average Loss: 2.5848\n",
      "GPU Memory: 159.31 MB\n",
      "\n",
      "Calculating training perplexity...\n",
      "Perplexity scores saved to models_rnn/train_perplexities_model_20250214_173419_8.txt\n",
      "Average Perplexity: 14.2284\n",
      "\n",
      "Calculating test perplexity...\n",
      "Perplexity scores saved to models_rnn/test_perplexities_model_20250214_173419_8.txt\n",
      "Average Perplexity: 357.9562\n",
      "\n",
      "Best Model:\n",
      "{\n",
      "    \"model_id\": \"model_20250214_171849_6\",\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 256,\n",
      "        \"embedding_dim\": 200,\n",
      "        \"epochs\": 5,\n",
      "        \"hidden_dim\": 512\n",
      "    },\n",
      "    \"train_perplexity\": 26.026884078979492,\n",
      "    \"test_perplexity\": 259.68768310546875,\n",
      "    \"model_path\": \"models_rnn/model_model_20250214_171849_6.pt\"\n",
      "}\n",
      "\n",
      "Training Summary:\n",
      "Total models trained: 9\n",
      "\n",
      "Best performing model:\n",
      "Model ID: model_20250214_171849_6\n",
      "Hyperparameters: {'batch_size': 256, 'embedding_dim': 200, 'epochs': 5, 'hidden_dim': 512}\n",
      "Train Perplexity: 26.0269\n",
      "Test Perplexity: 259.6877\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def train_and_evaluate_models(train_text, test_text, hyperparameters, base_model_dir=\"models\", results_file=\"model_results.json\"):\n",
    "    \"\"\"\n",
    "    Train multiple models with different hyperparameter combinations and evaluate their perplexities\n",
    "    \n",
    "    Args:\n",
    "        train_text: Training data (list of tokenized sentences)\n",
    "        test_text: Test data (list of tokenized sentences)\n",
    "        hyperparameters: Dictionary of hyperparameter lists to try\n",
    "        base_model_dir: Directory to save models\n",
    "        results_file: File to save results\n",
    "    \"\"\"\n",
    "    # Create model directory if it doesn't exist\n",
    "    os.makedirs(base_model_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate all combinations of hyperparameters\n",
    "    param_names = sorted(hyperparameters.keys())\n",
    "    param_values = [hyperparameters[name] for name in param_names]\n",
    "    param_combinations = list(itertools.product(*param_values))\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Train and evaluate each combination\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        param_dict = dict(zip(param_names, params))\n",
    "        print(f\"\\nTraining model {i+1}/{len(param_combinations)}\")\n",
    "        print(\"Parameters:\", param_dict)\n",
    "        \n",
    "        # Create model identifier\n",
    "        model_id = f\"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}\"\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model, dataset, trainer = train_language_model(\n",
    "                train_text,\n",
    "                embedding_dim=param_dict['embedding_dim'],\n",
    "                hidden_dim=param_dict['hidden_dim'],\n",
    "                batch_size=param_dict['batch_size'],\n",
    "                num_epochs=param_dict['epochs']\n",
    "            )\n",
    "            \n",
    "            # Calculate perplexities\n",
    "            print(\"\\nCalculating training perplexity...\")\n",
    "            train_perplexity = calculate_perplexity(\n",
    "                model=model,\n",
    "                sentences=train_text,\n",
    "                dataset=dataset,\n",
    "                device=device,\n",
    "                output_file=f\"{base_model_dir}/train_perplexities_{model_id}.txt\"\n",
    "            )\n",
    "            \n",
    "            print(\"\\nCalculating test perplexity...\")\n",
    "            test_perplexity = calculate_perplexity(\n",
    "                model=model,\n",
    "                sentences=test_text,\n",
    "                dataset=dataset,\n",
    "                device=device,\n",
    "                output_file=f\"{base_model_dir}/test_perplexities_{model_id}.txt\"\n",
    "            )\n",
    "            \n",
    "            # Save model\n",
    "            model_path = f\"{base_model_dir}/model_{model_id}.pt\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'hyperparameters': param_dict,\n",
    "                'train_perplexity': train_perplexity,\n",
    "                'test_perplexity': test_perplexity\n",
    "            }, model_path)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'model_id': model_id,\n",
    "                'hyperparameters': param_dict,\n",
    "                'train_perplexity': train_perplexity,\n",
    "                'test_perplexity': test_perplexity,\n",
    "                'model_path': model_path\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Save results after each model (in case of crashes)\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model with parameters {param_dict}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Find best model based on test perplexity\n",
    "    best_model = min(results, key=lambda x: x['test_perplexity'])\n",
    "    print(\"\\nBest Model:\")\n",
    "    print(json.dumps(best_model, indent=4))\n",
    "    \n",
    "    return results, best_model\n",
    "\n",
    "# Define hyperparameter grid\n",
    "hyperparameters = {\n",
    "    'embedding_dim': [50, 100, 200],\n",
    "    'hidden_dim': [512],\n",
    "    'batch_size': [256],\n",
    "    'epochs': [5, 6, 7]\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results, best_model = train_and_evaluate_models(\n",
    "    train_text=train_text,\n",
    "    test_text=test_text,\n",
    "    hyperparameters=hyperparameters,\n",
    "    base_model_dir=f\"models_rnn_{corp}\",\n",
    "    results_file=f\"model_results_rnn_{corp}.json\"\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Total models trained: {len(results)}\")\n",
    "print(\"\\nBest performing model:\")\n",
    "print(f\"Model ID: {best_model['model_id']}\")\n",
    "print(\"Hyperparameters:\", best_model['hyperparameters'])\n",
    "print(f\"Train Perplexity: {best_model['train_perplexity']:.4f}\")\n",
    "print(f\"Test Perplexity: {best_model['test_perplexity']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
