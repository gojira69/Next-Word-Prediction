{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS CLEANING, TOKENIZATION AND BUILDING VOCAB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPrideAndPrejudice(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    start_idx = text.find(\"CHAPTER I.\")\n",
    "    if start_idx == -1:\n",
    "        return text\n",
    "    text = text[start_idx:]\n",
    "\n",
    "    end_idx = text.find(\"Transcriber's note:\")\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "\n",
    "    text = re.sub(r\"CHAPTER\\s+[IVXLCDM]+\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleanUllyeses(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    first_idx = text.find(\"— I —\")\n",
    "    if first_idx == -1:\n",
    "        return text\n",
    "\n",
    "    second_idx = text.find(\"— I —\", first_idx + 1)\n",
    "    if second_idx == -1:\n",
    "        return text\n",
    "    text = text[second_idx:]\n",
    "\n",
    "    text = re.sub(r\"—\\s+[I|II|III]+\\s+—\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\[\\s*\\d+\\s*\\]\", \"\", text)\n",
    "\n",
    "    end_idx = text.find(\"Trieste-Zurich-Paris\")\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "\n",
    "    return text\n",
    "\n",
    "def Tokenizer(inputText):\n",
    "    text = inputText.lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"http\\S+\", \"<URL>\", text)\n",
    "    text = re.sub(r\"www\\S+\", \"<URL>\", text)\n",
    "    text = re.sub(r\"[A-Za-z0-9._%+-]+@[A-za-z0-9.-]+\\.[a-z]{2,}\", \"<MAILID>\", text)\n",
    "\n",
    "    text = re.sub(r\"[^\\@\\#\\.\\w\\?\\!\\s:-]\", \"\", text)\n",
    "    text = re.sub(f\"-\", \" \", text)\n",
    "    text = re.sub(r\"_\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\n*\", \"\", text)\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "\n",
    "    abbreviations = re.findall(r\"\\b([A-Z]([a-z]){,2}\\.)\", text)\n",
    "    if abbreviations:\n",
    "        abbreviations_set = set((list(zip(*abbreviations))[0]))\n",
    "\n",
    "        for word in abbreviations_set:\n",
    "            pattern = r\"\\b\" + re.escape(word)\n",
    "            text = re.sub(pattern, word.strip(\".\"), text)\n",
    "\n",
    "    text = re.sub(r\"#\\w+\\b\", \"<HASHTAG>\", text)\n",
    "    text = re.sub(r\"@\\w+\\b\", \"<MENTION>\", text)\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"<NUM>\", text)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    sentences = [\n",
    "        sentence.strip() for sentence in re.split(r\"[.!?:]+\", text) if sentence.strip()\n",
    "    ]\n",
    "    sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def trainTestSplit(sentences, testSize):\n",
    "    random.seed(69)\n",
    "\n",
    "    testSplit = random.sample(sentences, testSize)\n",
    "    duplicateTestSplit = testSplit.copy()\n",
    "    trainSplit = [\n",
    "        sentence\n",
    "        for sentence in sentences\n",
    "        if sentence not in duplicateTestSplit or duplicateTestSplit.remove(sentence)\n",
    "    ]\n",
    "\n",
    "    return trainSplit, testSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = input(\"Enter Corpus Path: \")\n",
    "# n = input(\"Enter value of N: \")\n",
    "\n",
    "file_path = 'corpus/corpus_1.txt'\n",
    "filteredText = \"\"\n",
    "corp = \"\"\n",
    "\n",
    "# corpus cleaning and tokenization\n",
    "if 'corpus_1.txt' in file_path:\n",
    "    filteredText = cleanPrideAndPrejudice(file_path)\n",
    "    corp = \"papc\"\n",
    "elif 'corpus_2.txt' in file_path:\n",
    "    filteredText = cleanUllyeses(file_path)\n",
    "    corp = \"uc\"\n",
    "else:\n",
    "    print(\"Corpus doesn't exist!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = Tokenizer(filteredText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text = trainTestSplit(sentences, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_text))\n",
    "print(len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(word for sentence in train_text for word in sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab.keys())}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "word_to_idx[\"<UNK>\"] = len(word_to_idx)\n",
    "idx_to_word[len(idx_to_word)] = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vocab Size: {len(word_to_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_sentences):\n",
    "        self.seq_length = max(len(sentence) for sentence in tokenized_sentences)\n",
    "        \n",
    "        # Flatten list of lists for vocabulary creation\n",
    "        all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "        \n",
    "        # Create vocabulary\n",
    "        word_counts = Counter(all_words)\n",
    "        self.vocab = ['<PAD>', '<UNK>'] + list(word_counts.keys())\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        # Convert sentences to indices and flatten\n",
    "        self.data = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            sentence_indices = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in sentence]\n",
    "            self.data.extend(sentence_indices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.data) - self.seq_length - 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx:idx + self.seq_length]\n",
    "        target = self.data[idx + 1:idx + self.seq_length + 1]\n",
    "        return torch.tensor(sequence), torch.tensor(target)\n",
    "    \n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        if hidden is None:\n",
    "            # LSTM needs both hidden state and cell state\n",
    "            h0 = torch.zeros(1, batch_size, self.hidden_dim, device=x.device)\n",
    "            c0 = torch.zeros(1, batch_size, self.hidden_dim, device=x.device)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_dim, device=device)\n",
    "        return (h0, c0)\n",
    "\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(model.parameters())\n",
    "        print(f\"Model moved to {device}\")\n",
    "\n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            hidden = self.model.init_hidden(data.size(0))\n",
    "            output, _ = self.model(data, hidden)\n",
    "\n",
    "            loss = self.criterion(\n",
    "                output.reshape(-1, output.size(-1)), target.reshape(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def predict_next_words(self, input_tokens, dataset, k=5):\n",
    "        \"\"\"\n",
    "        Predict next words given a list of input tokens\n",
    "        Args:\n",
    "            input_tokens: List of tokens or space-separated string\n",
    "            dataset: TextDataset instance\n",
    "            k: Number of top predictions to return\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # Handle both string and list input\n",
    "        if isinstance(input_tokens, str):\n",
    "            tokens = input_tokens.strip().split()\n",
    "        else:\n",
    "            tokens = input_tokens\n",
    "\n",
    "        # Convert tokens to indices\n",
    "        word_indices = [\n",
    "            dataset.word2idx.get(word, dataset.word2idx[\"<UNK>\"]) for word in tokens\n",
    "        ]\n",
    "\n",
    "        # # Trim sequence if longer than seq_length\n",
    "        # if len(word_indices) > dataset.seq_length:\n",
    "        #     word_indices = word_indices[-dataset.seq_length :]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_tensor = torch.tensor(word_indices).unsqueeze(0).to(device)\n",
    "            hidden = self.model.init_hidden(1)\n",
    "            output, _ = self.model(input_tensor, hidden)\n",
    "\n",
    "            probabilities = torch.softmax(output[0, -1], dim=0)\n",
    "            top_k_probs, top_k_indices = torch.topk(probabilities, k)\n",
    "\n",
    "            predictions = []\n",
    "            for prob, idx in zip(\n",
    "                top_k_probs.cpu().numpy(), top_k_indices.cpu().numpy()\n",
    "            ):\n",
    "                word = dataset.idx2word[idx]\n",
    "                predictions.append((word, prob))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def train_language_model(\n",
    "    tokenized_sentences, embedding_dim, hidden_dim, seq_length, batch_size, num_epochs\n",
    "):\n",
    "    # Print memory usage before training\n",
    "    if torch.cuda.is_available():\n",
    "        print(\n",
    "            f\"GPU Memory before training: {torch.cuda.memory_allocated()/1024**2:.2f} MB\"\n",
    "        )\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = TextDataset(tokenized_sentences, seq_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(f\"Vocabulary size: {len(dataset.vocab)}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = LSTM(len(dataset.vocab), embedding_dim, hidden_dim)\n",
    "    trainer = LanguageModelTrainer(model)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        loss = trainer.train_epoch(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {loss:.4f}\")\n",
    "\n",
    "        # Print memory usage after each epoch\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "\n",
    "    return model, dataset, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyperparams\n",
    "# embedding_dimensions = 50\n",
    "# hidden_dimensions = 128\n",
    "# sequence_length = 10\n",
    "# batch_size = 256\n",
    "# epochs = 10\n",
    "\n",
    "# model, dataset, trainer = train_language_model(\n",
    "#     train_text,\n",
    "#     embedding_dimensions,\n",
    "#     hidden_dimensions,\n",
    "#     sequence_length,\n",
    "#     batch_size,\n",
    "#     epochs,\n",
    "# )\n",
    "\n",
    "# print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using string\n",
    "# predictions = trainer.predict_next_words(\"mr darcy is the real object of her curiosity\", dataset, k=5)\n",
    "\n",
    "# print(\"\\nTest predictions:\")\n",
    "# for word, prob in predictions:\n",
    "#     print(f'Word: {word}, Probability: {prob:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, sentences, dataset, device, batch_size=64, output_file=\"perplexities.txt\"):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of tokenized sentences using batch processing\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_nll = 0\n",
    "    total_words = 0\n",
    "    perplexities = []\n",
    "    \n",
    "    # Process sentences in batches\n",
    "    with torch.no_grad():\n",
    "        # Convert all sentences to tensors first\n",
    "        batch_data = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if len(sentence) < 2:  # Skip too short sentences\n",
    "                continue\n",
    "                \n",
    "            indices = [dataset.word2idx.get(word, dataset.word2idx['<UNK>']) for word in sentence]\n",
    "            batch_data.append((i, indices))\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(batch_data), batch_size):\n",
    "            batch = batch_data[i:i + batch_size]\n",
    "            batch_indices = []\n",
    "            batch_targets = []\n",
    "            sentence_lengths = []\n",
    "            original_indices = []\n",
    "            \n",
    "            # Prepare batch data\n",
    "            for idx, indices in batch:\n",
    "                original_indices.append(idx)\n",
    "                batch_indices.append(indices[:-1])  # All but last word\n",
    "                batch_targets.append(indices[1:])   # All but first word\n",
    "                sentence_lengths.append(len(indices) - 1)\n",
    "            \n",
    "            # Pad sequences in batch\n",
    "            max_len = max(sentence_lengths)\n",
    "            padded_indices = [seq + [dataset.word2idx['<PAD>']] * (max_len - len(seq)) for seq in batch_indices]\n",
    "            padded_targets = [seq + [dataset.word2idx['<PAD>']] * (max_len - len(seq)) for seq in batch_targets]\n",
    "            \n",
    "            # Convert to tensors\n",
    "            context_tensor = torch.tensor(padded_indices).to(device)\n",
    "            target_tensor = torch.tensor(padded_targets).to(device)\n",
    "            lengths_tensor = torch.tensor(sentence_lengths).to(device)\n",
    "            \n",
    "            # Get model output\n",
    "            output, _ = model(context_tensor)\n",
    "            \n",
    "            # Calculate log probabilities\n",
    "            log_probs = torch.log_softmax(output, dim=-1)\n",
    "            \n",
    "            # Calculate perplexity for each sentence in batch\n",
    "            for j in range(len(batch)):\n",
    "                length = sentence_lengths[j]\n",
    "                sentence_log_probs = log_probs[j, :length]\n",
    "                sentence_targets = target_tensor[j, :length]\n",
    "                \n",
    "                # Calculate NLL for this sentence\n",
    "                nll = 0\n",
    "                for t in range(length):\n",
    "                    nll += -sentence_log_probs[t, sentence_targets[t]]\n",
    "                \n",
    "                # Calculate perplexity\n",
    "                ppl_score = torch.exp(nll / length).item()\n",
    "                perplexities.append(f\"sentence {original_indices[j]+1}: {ppl_score:.4f}\")\n",
    "                \n",
    "                # Update totals\n",
    "                total_nll += nll.item()\n",
    "                total_words += length\n",
    "            \n",
    "    # Write results to file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(perplexities))\n",
    "    print(f\"Perplexity scores saved to {output_file}\")\n",
    "    \n",
    "    # Calculate average perplexity\n",
    "    if total_words > 0:\n",
    "        avg_nll = total_nll / total_words\n",
    "        avg_perplexity = torch.exp(torch.tensor(avg_nll)).item()\n",
    "        print(f\"Average Perplexity: {avg_perplexity:.4f}\")\n",
    "        return avg_perplexity\n",
    "    else:\n",
    "        print(\"No valid sentences found for perplexity calculation\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate perplexities for both sets\n",
    "# print(\"Calculating training set perplexity...\")\n",
    "# train_perplexity = calculate_perplexity(\n",
    "#     model=model,\n",
    "#     sentences=train_text,  # Your training sentences\n",
    "#     dataset=dataset,\n",
    "#     device=device,\n",
    "#     batch_size=128,  # Adjust based on your GPU memory\n",
    "#     output_file=\"train_perplexities.txt\"\n",
    "# )\n",
    "\n",
    "# print(\"\\nCalculating test set perplexity...\")\n",
    "# test_perplexity = calculate_perplexity(\n",
    "#     model=model,\n",
    "#     sentences=test_text,  # Your test sentences\n",
    "#     dataset=dataset,\n",
    "#     device=device,\n",
    "#     batch_size=128,  # Adjust based on your GPU memory\n",
    "#     output_file=\"test_perplexities.txt\"\n",
    "# )\n",
    "\n",
    "# print(f\"\\nFinal Results:\")\n",
    "# print(f\"Training Set Perplexity: {train_perplexity:.4f}\")\n",
    "# print(f\"Test Set Perplexity: {test_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def train_and_evaluate_models(train_text, test_text, hyperparameters, base_model_dir=\"models_lstm\", results_file=\"model_results_lstm.json\"):\n",
    "    \"\"\"\n",
    "    Train multiple models with different hyperparameter combinations and evaluate their perplexities\n",
    "    \n",
    "    Args:\n",
    "        train_text: Training data (list of tokenized sentences)\n",
    "        test_text: Test data (list of tokenized sentences)\n",
    "        hyperparameters: Dictionary of hyperparameter lists to try\n",
    "        base_model_dir: Directory to save models\n",
    "        results_file: File to save results\n",
    "    \"\"\"\n",
    "    # Create model directory if it doesn't exist\n",
    "    os.makedirs(base_model_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate all combinations of hyperparameters\n",
    "    param_names = sorted(hyperparameters.keys())\n",
    "    param_values = [hyperparameters[name] for name in param_names]\n",
    "    param_combinations = list(itertools.product(*param_values))\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Train and evaluate each combination\n",
    "    for i, params in enumerate(param_combinations):\n",
    "        param_dict = dict(zip(param_names, params))\n",
    "        print(f\"\\nTraining model {i+1}/{len(param_combinations)}\")\n",
    "        print(\"Parameters:\", param_dict)\n",
    "        \n",
    "        # Create model identifier\n",
    "        model_id = f\"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}\"\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model, dataset, trainer = train_language_model(\n",
    "                train_text,\n",
    "                embedding_dim=param_dict['embedding_dim'],\n",
    "                hidden_dim=param_dict['hidden_dim'],\n",
    "                seq_length=param_dict['seq_length'],\n",
    "                batch_size=param_dict['batch_size'],\n",
    "                num_epochs=param_dict['epochs']\n",
    "            )\n",
    "            \n",
    "            # Calculate perplexities\n",
    "            print(\"\\nCalculating training perplexity...\")\n",
    "            train_perplexity = calculate_perplexity(\n",
    "                model=model,\n",
    "                sentences=train_text,\n",
    "                dataset=dataset,\n",
    "                device=device,\n",
    "                output_file=f\"{base_model_dir}/train_perplexities_{model_id}.txt\"\n",
    "            )\n",
    "            \n",
    "            print(\"\\nCalculating test perplexity...\")\n",
    "            test_perplexity = calculate_perplexity(\n",
    "                model=model,\n",
    "                sentences=test_text,\n",
    "                dataset=dataset,\n",
    "                device=device,\n",
    "                output_file=f\"{base_model_dir}/test_perplexities_{model_id}.txt\"\n",
    "            )\n",
    "            \n",
    "            # Save model\n",
    "            model_path = f\"{base_model_dir}/model_{model_id}.pt\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'hyperparameters': param_dict,\n",
    "                'train_perplexity': train_perplexity,\n",
    "                'test_perplexity': test_perplexity\n",
    "            }, model_path)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'model_id': model_id,\n",
    "                'hyperparameters': param_dict,\n",
    "                'train_perplexity': train_perplexity,\n",
    "                'test_perplexity': test_perplexity,\n",
    "                'model_path': model_path\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Save results after each model (in case of crashes)\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model with parameters {param_dict}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Find best model based on test perplexity\n",
    "    best_model = min(results, key=lambda x: x['test_perplexity'])\n",
    "    print(\"\\nBest Model:\")\n",
    "    print(json.dumps(best_model, indent=4))\n",
    "    \n",
    "    return results, best_model\n",
    "\n",
    "# Define hyperparameter grid\n",
    "hyperparameters = {\n",
    "    'embedding_dim': [100],\n",
    "    'hidden_dim': [512],\n",
    "    'seq_length': [40],\n",
    "    'batch_size': [128],\n",
    "    'epochs': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results, best_model = train_and_evaluate_models(\n",
    "    train_text=train_text,\n",
    "    test_text=test_text,\n",
    "    hyperparameters=hyperparameters,\n",
    "    base_model_dir=f\"models_lstm_{corp}\",\n",
    "    results_file=f\"model_results_lstm_{corp}.json\"\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Total models trained: {len(results)}\")\n",
    "print(\"\\nBest performing model:\")\n",
    "print(f\"Model ID: {best_model['model_id']}\")\n",
    "print(\"Hyperparameters:\", best_model['hyperparameters'])\n",
    "print(f\"Train Perplexity: {best_model['train_perplexity']:.4f}\")\n",
    "print(f\"Test Perplexity: {best_model['test_perplexity']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
